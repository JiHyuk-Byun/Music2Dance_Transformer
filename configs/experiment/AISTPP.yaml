# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /trainer    : null
  - override /datamodule : null
  - override /model      : null
  - override /callbacks  : null
  - override /logger     : null

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

# name of the run determines folder name in logs
name: "Aist++_M2D_baseline"

seed: 12345

timestamp: "${now:%Y-%m-%d}_${now:%H-%M-%S}"
server_name: "${oc.env:SERVER_NAME}"
gpu_name: "${oc.env:GPU_NAME}"
root_dir: "/workspace/ssd1/users/jhbyun/aist/"

trainer:
  _target_                  : pytorch_lightning.Trainer
  accelerator               : gpu
  devices                   : [0]
  min_epochs                : 1
  max_epochs                : 2
  auto_lr_find              : False

  gradient_clip_val         : 0
  accumulate_grad_batches   : 1
  check_val_every_n_epoch   : 1000

  #weights_summary           : "top"
  #progress_bar_refresh_rate : 10

  fast_dev_run              : False
  resume_from_checkpoint    : False

datamodule:
  _target_         : datamodules.aist_datamodule.AISTPPDataModule
  data_path        : ${data_dir}/aistplusplus
  music_length     : ${model.gen_params.music_length}
  seed_m_length    : ${model.gen_params.seed_m_length}
  predict_length   : ${model.gen_params.predict_length}
  train_test_split : [ 1333, 30 ] # [1368, 40] ignore unnecessary datas
  batch_size       : 16
  num_workers      : 16
  pin_memory       : True

model:
  _target_  : models.aist_module.AISTLitModule

  gen_params:
    audio_channel     : 441
    noise_size        : 256
    dim               : 512
    depth             : 4
    heads             : 8
    mlp_dim           : 2048
    music_length      : 240
    seed_m_length     : 120
    predict_length    : 30
    rot_6d            : True
    device            : None
    smpl              : ${data_dir}/smpl
    reconstruction    : True
    cross_attn        : False
    mint              : False

  loss_params:
    rot_6d: ${model.gen_params.rot_6d}
    alphas: [0.636, 2.964]


  optimizer:
    type: AdamW
    kwargs:
      lr           : 1e-4
      betas        : [0.9, 0.99]
      weight_decay : 0

  scheduler:
    type: MultiStepLR
    kwargs:
      milestones : [400, 1000]
      gamma      : 0.1

callbacks:
  model_checkpoint:
    _target_            : pytorch_lightning.callbacks.ModelCheckpoint
    dirpath             : "${hydra:runtime.output_dir}/checkpoints"
    filename            : null
    save_last           : True
    save_top_k          : -1
    every_n_train_steps : 50

  dance_generation:
    _target_      : callbacks.dance_generation.DanceGeneration
    data_path     : ${datamodule.data_path}
    data_name     : gBR_sBM_cAll_d04_mBR0_ch01.pkl
    seed_m_length : ${model.gen_params.seed_m_length}
    play_time     : 10
    num_sample    : 2
    smpl_path     : ${data_dir}/smpl

logger:
  wandb:
    _target_: pytorch_lightning.loggers.wandb.WandbLogger
    # name: "" # name of the run (normally generated by wandb)
    name: "${timestamp}_${server_name}_${gpu_name}" # name of the run (normally generated by wandb)
    save_dir: "${hydra:runtime.output_dir}"
    offline: False
    id: null # pass correct id to resume experiment!
    anonymous: null # enable anonymous logging
    project: "aistplusplus"
    log_model: False # upload lightning ckpts
    prefix: "" # a string to put at the beginning of metric keys
    # entity: "" # set to name of your wandb team
    group: "${name}"
    job_type: ""

hydra:
  run:
    dir: ${root_dir}/logs/runs/${name}/${logger.wandb.name}